{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI4Code: data-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clear data:\n",
    "- html tags\n",
    "    for example:\n",
    "    - <#div>\n",
    "    - <#br>\n",
    "    - <img alt=\"\"\"\" src=\"\"data..base64..>\n",
    "    - <img src=\"\"*>\n",
    "- math formulas\n",
    "- links\n",
    "- emails\n",
    "- ![image]#(attachment)\n",
    "- not ascii characters(emoji or else)\n",
    "\n",
    "translate non engilsh markdowns to engilsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(document):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r\"\\W\", \" \", str(document))\n",
    "\n",
    "    # remove all single characters\n",
    "    document = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", document)\n",
    "\n",
    "    # Remove new line simbols for language identification\n",
    "    document = document.replace(\"\\n\", \" \")\n",
    "\n",
    "    # Remove html tags\n",
    "    document = re.sub(r\"<.*?>\", \"\", document)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r\"\\^[a-zA-Z]\\s+\", \" \", document)\n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r\"\\s+\", \" \", document, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r\"^b\\s+\", \"\", document)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "\n",
    "    # remove digits\n",
    "    document = re.sub(r\"[0-9]+\", \"\", document)\n",
    "\n",
    "    # Lemmatization\n",
    "    #tokens = document.split()\n",
    "    #tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "    #tokens = [word for word in tokens if len(word) > 2]\n",
    "\n",
    "    #preprocessed_text = \" \".join(tokens)\n",
    "    return document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = '''\"# [Forest Cover Type Multi-class Classification](https://www.kaggle.com/uciml/forest-cover-type-dataset)<a class=\"\"tocSkip\"\"/>\"\n",
    "\"<h1>Table of Contents<span class=\"\"tocSkip\"\"></span></h1>\n",
    "<div class=\"\"toc\"\"><ul class=\"\"toc-item\"\"><li><span><a href=\"\"#Introduction\"\" data-toc-modified-id=\"\"Introduction-1\"\">Introduction</a></span><ul class=\"\"toc-item\"\"><li><span><a href=\"\"#Features-explanation\"\" data-toc-modified-id=\"\"Features-explanation-1.1\"\">Features explanation</a></span></li><li><span><a href=\"\"#Reversing-one-hot-encoding-to-label-encoding\"\" data-toc-modified-id=\"\"Reversing-one-hot-encoding-to-label-encoding-1.2\"\">Reversing one-hot encoding to label encoding</a></span></li></ul></li><li><span><a href=\"\"#Exploratory-Data-Analysis\"\" data-toc-modified-id=\"\"Exploratory-Data-Analysis-2\"\">Exploratory Data Analysis</a></span><ul class=\"\"toc-item\"\"><li><span><a href=\"\"#Cover-Type-distribution\"\" data-toc-modified-id=\"\"Cover-Type-distribution-2.1\"\">Cover Type distribution</a></span></li><li><span><a href=\"\"#Numerical-Features\"\" data-toc-modified-id=\"\"Numerical-Features-2.2\"\">Numerical Features</a></span><ul class=\"\"toc-item\"\"><li><span><a href=\"\"#Correlation-Analysis\"\" data-toc-modified-id=\"\"Correlation-Analysis-2.2.1\"\">Correlation Analysis</a></span></li><li><span><a href=\"\"#Inputting-Hillshade_3pm-with-Random-Forest-Classifier\"\" data-toc-modified-id=\"\"Inputting-Hillshade_3pm-with-Random-Forest-Classifier-2.2.2\"\">Inputting Hillshade_3pm with Random Forest Classifier</a></span></li><li><span><a href=\"\"#Features-distribution\"\" data-toc-modified-id=\"\"Features-distribution-2.2.3\"\">Features distribution</a></span></li></ul></li><li><span><a href=\"\"#Categorical-Features\"\" data-toc-modified-id=\"\"Categorical-Features-2.3\"\">Categorical Features</a></span><ul class=\"\"toc-item\"\"><li><span><a href=\"\"#Features-distribution\"\" data-toc-modified-id=\"\"Features-distribution-2.3.1\"\">Features distribution</a></span></li><li><span><a href=\"\"#Chi-square-test-for-categorical-data\"\" data-toc-modified-id=\"\"Chi-square-test-for-categorical-data-2.3.2\"\">Chi-square test for categorical data</a></span></li></ul></li></ul></li><li><span><a href=\"\"#Model-Selection\"\" data-toc-modified-id=\"\"Model-Selection-3\"\">Model Selection</a></span><ul class=\"\"toc-item\"\"><li><span><a href=\"\"#Model-Evaluation\"\" data-toc-modified-id=\"\"Model-Evaluation-3.1\"\">Model Evaluation</a></span><ul class=\"\"toc-item\"\"><li><span><a href=\"\"#F1-score\"\" data-toc-modified-id=\"\"F1-score-3.1.1\"\">F1-score</a></span></li><li><span><a href=\"\"#Normalized-Confusion-Matrix\"\" data-toc-modified-id=\"\"Normalized-Confusion-Matrix-3.1.2\"\">Normalized Confusion Matrix</a></span></li><li><span><a href=\"\"#Stratified-k-fold-cross-validation\"\" data-toc-modified-id=\"\"Stratified-k-fold-cross-validation-3.1.3\"\">Stratified k-fold cross-validation</a></span></li><li><span><a href=\"\"#Learning-curve\"\" data-toc-modified-id=\"\"Learning-curve-3.1.4\"\">Learning curve</a></span></li></ul></li><li><span><a href=\"\"#Logistic-Regression\"\" data-toc-modified-id=\"\"Logistic-Regression-3.2\"\">Logistic Regression</a></span><ul class=\"\"toc-item\"\"><li><span><a href=\"\"#Feature-selection-with-RFE\"\" data-toc-modified-id=\"\"Feature-selection-with-RFE-3.2.1\"\">Feature selection with RFE</a></span></li><li><span><a href=\"\"#Polynomial-logistic-regression\"\" data-toc-modified-id=\"\"Polynomial-logistic-regression-3.2.2\"\">Polynomial logistic regression</a></span></li><li><span><a href=\"\"#Resampling\"\" data-toc-modified-id=\"\"Resampling-3.2.3\"\">Resampling</a></span></li></ul></li><li><span><a href=\"\"#KNN\"\" data-toc-modified-id=\"\"KNN-3.3\"\">KNN</a></span><ul class=\"\"toc-item\"\"><li><span><a href=\"\"#Dimensionality-reduction-with-PCA\"\" data-toc-modified-id=\"\"Dimensionality-reduction-with-PCA-3.3.1\"\">Dimensionality reduction with PCA</a></span></li><li><span><a href=\"\"#Hyperparameters-tuning\"\" data-toc-modified-id=\"\"Hyperparameters-tuning-3.3.2\"\">Hyperparameters tuning</a></span></li></ul></li><li><span><a href=\"\"#Decision-Tree-Classifier\"\" data-toc-modified-id=\"\"Decision-Tree-Classifier-3.4\"\">Decision Tree Classifier</a></span><ul class=\"\"toc-item\"\"><li><span><a href=\"\"#Feature-importance\"\" data-toc-modified-id=\"\"Feature-importance-3.4.1\"\">Feature importance</a></span></li><li><span><a href=\"\"#Hyperparameters-tuning\"\" data-toc-modified-id=\"\"Hyperparameters-tuning-3.4.2\"\">Hyperparameters tuning</a></span></li></ul></li><li><span><a href=\"\"#Bagging\"\" data-toc-modified-id=\"\"Bagging-3.5\"\">Bagging</a></span><ul class=\"\"toc-item\"\"><li><span><a href=\"\"#Hyperparameters-tuning\"\" data-toc-modified-id=\"\"Hyperparameters-tuning-3.5.1\"\">Hyperparameters tuning</a></span></li></ul></li><li><span><a href=\"\"#Random-Forest\"\" data-toc-modified-id=\"\"Random-Forest-3.6\"\">Random Forest</a></span><ul class=\"\"toc-item\"\"><li><span><a href=\"\"#Feature-importance\"\" data-toc-modified-id=\"\"Feature-importance-3.6.1\"\">Feature importance</a></span></li><li><span><a href=\"\"#Hyperparameters-tuning\"\" data-toc-modified-id=\"\"Hyperparameters-tuning-3.6.2\"\">Hyperparameters tuning</a></span></li></ul></li><li><span><a href=\"\"#Boosting\"\" data-toc-modified-id=\"\"Boosting-3.7\"\">Boosting</a></span><ul class=\"\"toc-item\"\"><li><span><a href=\"\"#Hyperparameters-tuning\"\" data-toc-modified-id=\"\"Hyperparameters-tuning-3.7.1\"\">Hyperparameters tuning</a></span></li><li><span><a href=\"\"#Further-improvement-with-feature-engineering\"\" data-toc-modified-id=\"\"Further-improvement-with-feature-engineering-3.7.2\"\">Further improvement with feature engineering</a></span></li></ul></li><li><span><a href=\"\"#Support-Vector-Machines\"\" data-toc-modified-id=\"\"Support-Vector-Machines-3.8\"\">Support Vector Machines</a></span><ul class=\"\"toc-item\"\"><li><span><a href=\"\"#SVC\"\" data-toc-modified-id=\"\"SVC-3.8.1\"\">SVC</a></span></li><li><span><a href=\"\"#SVM\"\" data-toc-modified-id=\"\"SVM-3.8.2\"\">SVM</a></span></li><li><span><a href=\"\"#Minibatch-Gradient-Descent-optimizer\"\" data-toc-modified-id=\"\"Minibatch-Gradient-Descent-optimizer-3.8.3\"\">Minibatch Gradient Descent optimizer</a></span></li><li><span><a href=\"\"#Hyperparameters-tuning\"\" data-toc-modified-id=\"\"Hyperparameters-tuning-3.8.4\"\">Hyperparameters tuning</a></span></li><li><span><a href=\"\"#Exact-SVM-on-undersampled-data\"\" data-toc-modified-id=\"\"Exact-SVM-on-undersampled-data-3.8.5\"\">Exact SVM on undersampled data</a></span></li></ul></li></ul></li><li><span><a href=\"\"#Conclusions\"\" data-toc-modified-id=\"\"Conclusions-4\"\">Conclusions</a></span><ul class=\"\"toc-item\"\"><li><span><a href=\"\"#Models-comparison\"\" data-toc-modified-id=\"\"Models-comparison-4.1\"\">Models comparison</a></span></li><li><span><a href=\"\"#Further-improvement\"\" data-toc-modified-id=\"\"Further-improvement-4.2\"\">Further improvement</a></span></li></ul></li><li><span><a href=\"\"#References\"\" data-toc-modified-id=\"\"References-5\"\">References</a></span></li></ul></div>\"\n",
    "# Introduction\n",
    "\"\"\"Forest Cover Type\"\" dataset contains tree observations from four areas of the Roosevelt National Forest in Colorado. All measurements are cartographic variables (no remote sensing) from 30 meter x 30 meter sections of forest and amount to over half a million.\"\n",
    "\"The dataset comprehends 54 cartographic variables plus the class label, 10 of which are quantitative features while the remaining 44 correspond to 2 qualitative variables one-hot encoded.\n",
    "In this work our goal is to classify the *cover type* based on predictor variables of each observation (30 x 30 meter cell).\n",
    "<br>Since we have 7 possible Cover Type in given areas, we are approaching a *multiclass classification problem*\"\n",
    "## Features explanation\n",
    "\"In this section, we briefly present the information regarding the meaning of each feature and its admissible values.\"\n",
    "\"- Elevation: Elevation in meters.\n",
    "- Aspect: Aspect in degrees azimuth.\n",
    "- Slope: Slope in degrees.\n",
    "- Horizontal_Distance_To_Hydrology: Horizontal distance in meters to nearest surface water features.\n",
    "- Vertical_Distance_To_Hydrology: Vertical distance in meters to nearest surface water features.\n",
    "- Horizontal_Distance_To_Roadways: Horizontal distance in meters to the nearest roadway.\n",
    "- Hillshade_9am: hillshade index at 9am, summer solstice. Value out of 255.\n",
    "- Hillshade_Noon: hillshade index at noon, summer solstice. Value out of 255.\n",
    "- Hillshade_3pm: shade index at 3pm, summer solstice. Value out of 255.\n",
    "- Horizontal_Distance_To_Fire_Point*: horizontal distance in meters to nearest wildfire ignition points.\n",
    "- Wilderness_Area#: wilderness area designation.\n",
    "- Soil_Type#: soil type designation.\"\n",
    "\"Wilderness_Area feature is one-hot encoded to 4 binary columns (0 = absence or 1 = presence), each of these corresponds to a wilderness area designation.\n",
    "<br>Areas are mapped to value in the following way:\n",
    "1. Rawah Wilderness Area\n",
    "2. Neota Wilderness Area\n",
    "3. Comanche Peak Wilderness Area\n",
    "4. Cache la Poudre Wilderness Area\"\n",
    "\"The same goes for Soil_Type feature which is encoded as 40 one-hot encoded binary columns (0 = absence or 1 = presence) and each of these represents soil type designation.\n",
    "<br>All the possible options are:'''\n",
    "\n",
    "test2 = '''e need to first contenate train set and test set so that we can perform transformations on them simultaneously.\"\n",
    "\"## Feature Extraction\n",
    "\n",
    "Feature extraction means that we combine existing features to produce a more useful one. There are a number of variables that are describing one aspect of a house, such as open porch, so we can combine them in some ways.\"\n",
    "\"### Total Living Area\n",
    "\n",
    "Based on the correlation matrix, GrLivArea is the second largest determinant, indicating that people places greater importance on the activity space. Since basement is also another kind of living space, I combine the two variables into one TotalLivingArea.\"\n",
    "\"**Additional Research**\n",
    "\n",
    "Based on information from websites, I found that the total area of a house can be splitted into several parts just as the picture shows:\n",
    "\n",
    "<img src=\"\"https://www.gimme-shelter.com/wp-content/uploads/2011/09/Total-Floor-Area-of-a-House.png\"\" width=\"\"300\"\">\n",
    "\n",
    "Assume that this is a house with two levels\n",
    "\n",
    "Total Floor Area(Above-grade Living Area) = Living Area + First Floor + Second Floor + Addition'''\n",
    "\n",
    "test3 = '''# All Life bank - loan modelling - supervised learning - classification\n",
    "\"<img alt=\"\"\"\" src=\"\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAcFBgYGBQcGBgYICAcJCxIMCwoKCxcQEQ0SGxccHBoXGhkdISokHR8oIBkaJTIlKCwtLzAvHSM0ODQuNyouLy7/2wBDAQgICAsKCxYMDBYuHhoeLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi7/wgARCANpCAADAREAAhEBAxEB/8QAHAAAAgMBAQEBAAAAAAAAAAAAAgMBBAUABgcI/8QAGwEBAQEBAQEBAQAAAAAAAAAAAAECAwQFBgf/2gAMAwEAAhADEAAAAPqeLNonDbmIrjhasQ4ipAUkgkmiBDSsLLJWpyjU5EJiBtg11So5KJGQAVIog1QhCI5VFeAK0vCCpLSzrEzastVTK0cFFima/TWX6S1tWdKFvc/LXF5LFCmIk6lSKxOABFsKuOkBlbmGuQXAXAOaXJV4hccnWHJ6zOPp+eX0G89beS1M+n7zd6QsXPl5aOKya5Miy0tSsKXpo4tJaShnTlu2ZmXmK9F0wRmtXmK6Y812eisa0FXcea03N4q2Zsu/c0paksV7T0cfqPt8+lzlvkfg/Djj5t5fsfJfjfqKpU3mrrNPeKbKNQU6gl+q+fXvPNm3eDu3P1P2/jExofuOMUl5KzbLVLt0UUXiylrDseL929FOsVr5dxHdGzQj6F5K7a2XihCG818Ghtn2UvYfc/YXwK9h+l9y7kIXYv4qyIsv2G+pXhY1ilZ9d+hXR4QxvTN6wx+ex+2mrZQ0y43h/I0K8ays32UMseKPrDL+BeUo+5fyWP2PxfwmP+VpNicmzeE3vtsrt2KijRSxv39bN0JDlHD9miK5Z0/DOmuWRfDE+PatFSxRvvr2F3Ne7Sst4/H9YsKn317j/AJVRi28b7NjH7KRZqheMbr32pNlj4whdrIdPlklwiUntlbHyh3yOPk6kVpjupEOp7Gi9jUsWrK49i80PP3jx8CoirRs9Kr9dz8zX8Z+HbvGuzRzjed53jZs3jZr29HHfo/Fn5vOj8TZs0bNo17GjZo/E17ujWNd2va2fjjaNfrtv4n//2Q==\"\" />\"\n",
    "\"## Context\n",
    "\n",
    "All Life Bank is a US bank that has a growing customer base. The majority of these customers are liability customers (depositors) with varying sizes of deposits. The number of customers who are also borrowers (asset customers) is quite small, and the bank is interested in expanding this base rapidly to bring in more loan business and in the process, earn more through the interest on loans. In particular, the management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors).\n",
    "\n",
    "A campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio.\n",
    "\n",
    "You as a Data scientist at AllLife bank have to build a model that will help the marketing department to identify the potential customers who have a higher probability of purchasing the loan.\n",
    "\n",
    "## Objective\n",
    "The classification goal is to predict the likelihood of a liability customer buying personal loans which means we have to build a model which will be used to predict which customer will most likely to accept the offer for personal loan, based on the specific relationship with the bank across various features given in the dataset.\n",
    "\n",
    "### Key questions to be answered\n",
    "\n",
    "1. To predict whether a liability customer will buy a personal loan or not?\n",
    "2. Which variables are most significant?\n",
    "3. Which segment of customers should be targeted more?\n",
    "\n",
    "### Data Information\n",
    "The data contains the important demographic and banking details of the customers.\n",
    "\n",
    "**Attribute Information:**\n",
    "1. ID: Customer ID\n",
    "2. Age: Customer’s age in completed years\n",
    "3. Experience: #years of professional experience\n",
    "4. Income: Annual income of the customer (in thousand dollars)\n",
    "5. ZIP Code: Home Address ZIP code.\n",
    "6. Family: the Family size of the customer\n",
    "7. CCAvg: Average spending on credit cards per month (in thousand dollars)\n",
    "8. Education: Education Level. 1: Undergrad; 2: Graduate;3: Advanced/Professional\n",
    "9. Mortgage: Value of house mortgage if any. (in thousand dollars)\n",
    "10. Personal_Loan: Did this customer accept the personal loan offered in the last campaign?\n",
    "11. Securities_Account: Does the customer have securities account with the bank?\n",
    "12. CD_Account: Does the customer have a certificate of deposit (CD) account with the bank?\n",
    "13. Online: Do customers use internet banking facilities?\n",
    "14. CreditCard: Does the customer use a credit card issued by any other Bank (excluding All life Bank)?\n",
    "\"\n",
    "## Import required libraries\n",
    "## Define all required functions\n",
    "## Load the dataset in dataframe from the file\n",
    "## Understand data\n",
    "### Check the shape of data\n",
    "### List the columns\n",
    "### Check the datatypes of columns\n",
    "\"#### **Observations:**\n",
    "\n",
    "1. All columns are of number type (int64 / float64)\n",
    "2. There are no columns with null value\"\n",
    "### Check the missing data\n",
    "\"**Observations:**\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"# [Forest Cover Type Multi-class Classification](https://www.kaggle.com/uciml/forest-cover-type-dataset)\"\\n\"Table of Contents\\nIntroductionFeatures explanationReversing one-hot encoding to label encodingExploratory Data AnalysisCover Type distributionNumerical FeaturesCorrelation AnalysisInputting Hillshade_3pm with Random Forest ClassifierFeatures distributionCategorical FeaturesFeatures distributionChi-square test for categorical dataModel SelectionModel EvaluationF1-scoreNormalized Confusion MatrixStratified k-fold cross-validationLearning curveLogistic RegressionFeature selection with RFEPolynomial logistic regressionResamplingKNNDimensionality reduction with PCAHyperparameters tuningDecision Tree ClassifierFeature importanceHyperparameters tuningBaggingHyperparameters tuningRandom ForestFeature importanceHyperparameters tuningBoostingHyperparameters tuningFurther improvement with feature engineeringSupport Vector MachinesSVCSVMMinibatch Gradient Descent optimizerHyperparameters tuningExact SVM on undersampled dataConclusionsModels comparisonFurther improvementReferences\"\\n# Introduction\\n\"\"\"Forest Cover Type\"\" dataset contains tree observations from four areas of the Roosevelt National Forest in Colorado. All measurements are cartographic variables (no remote sensing) from 30 meter x 30 meter sections of forest and amount to over half a million.\"\\n\"The dataset comprehends 54 cartographic variables plus the class label, 10 of which are quantitative features while the remaining 44 correspond to 2 qualitative variables one-hot encoded.\\nIn this work our goal is to classify the *cover type* based on predictor variables of each observation (30 x 30 meter cell).\\nSince we have 7 possible Cover Type in given areas, we are approaching a *multiclass classification problem*\"\\n## Features explanation\\n\"In this section, we briefly present the information regarding the meaning of each feature and its admissible values.\"\\n\"- Elevation: Elevation in meters.\\n- Aspect: Aspect in degrees azimuth.\\n- Slope: Slope in degrees.\\n- Horizontal_Distance_To_Hydrology: Horizontal distance in meters to nearest surface water features.\\n- Vertical_Distance_To_Hydrology: Vertical distance in meters to nearest surface water features.\\n- Horizontal_Distance_To_Roadways: Horizontal distance in meters to the nearest roadway.\\n- Hillshade_9am: hillshade index at 9am, summer solstice. Value out of 255.\\n- Hillshade_Noon: hillshade index at noon, summer solstice. Value out of 255.\\n- Hillshade_3pm: shade index at 3pm, summer solstice. Value out of 255.\\n- Horizontal_Distance_To_Fire_Point*: horizontal distance in meters to nearest wildfire ignition points.\\n- Wilderness_Area#: wilderness area designation.\\n- Soil_Type#: soil type designation.\"\\n\"Wilderness_Area feature is one-hot encoded to 4 binary columns (0 = absence or 1 = presence), each of these corresponds to a wilderness area designation.\\nAreas are mapped to value in the following way:\\n1. Rawah Wilderness Area\\n2. Neota Wilderness Area\\n3. Comanche Peak Wilderness Area\\n4. Cache la Poudre Wilderness Area\"\\n\"The same goes for Soil_Type feature which is encoded as 40 one-hot encoded binary columns (0 = absence or 1 = presence) and each of these represents soil type designation.\\nAll the possible options are:'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = re.sub(r\"<.*?>\", \"\", test1)\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e need to first contenate train set and test set so that we can perform transformations on them simultaneously.\"\\n\"## Feature Extraction\\n\\nFeature extraction means that we combine existing features to produce a more useful one. There are a number of variables that are describing one aspect of a house, such as open porch, so we can combine them in some ways.\"\\n\"### Total Living Area\\n\\nBased on the correlation matrix, GrLivArea is the second largest determinant, indicating that people places greater importance on the activity space. Since basement is also another kind of living space, I combine the two variables into one TotalLivingArea.\"\\n\"**Additional Research**\\n\\nBased on information from websites, I found that the total area of a house can be splitted into several parts just as the picture shows:\\n\\n\\n\\nAssume that this is a house with two levels\\n\\nTotal Floor Area(Above-grade Living Area) = Living Area + First Floor + Second Floor + Addition'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = re.sub(r\"<.*?>\", \"\", test2)\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# All Life bank - loan modelling - supervised learning - classification\\n\"\"\\n\"## Context\\n\\nAll Life Bank is a US bank that has a growing customer base. The majority of these customers are liability customers (depositors) with varying sizes of deposits. The number of customers who are also borrowers (asset customers) is quite small, and the bank is interested in expanding this base rapidly to bring in more loan business and in the process, earn more through the interest on loans. In particular, the management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors).\\n\\nA campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio.\\n\\nYou as a Data scientist at AllLife bank have to build a model that will help the marketing department to identify the potential customers who have a higher probability of purchasing the loan.\\n\\n## Objective\\nThe classification goal is to predict the likelihood of a liability customer buying personal loans which means we have to build a model which will be used to predict which customer will most likely to accept the offer for personal loan, based on the specific relationship with the bank across various features given in the dataset.\\n\\n### Key questions to be answered\\n\\n1. To predict whether a liability customer will buy a personal loan or not?\\n2. Which variables are most significant?\\n3. Which segment of customers should be targeted more?\\n\\n### Data Information\\nThe data contains the important demographic and banking details of the customers.\\n\\n**Attribute Information:**\\n1. ID: Customer ID\\n2. Age: Customer’s age in completed years\\n3. Experience: #years of professional experience\\n4. Income: Annual income of the customer (in thousand dollars)\\n5. ZIP Code: Home Address ZIP code.\\n6. Family: the Family size of the customer\\n7. CCAvg: Average spending on credit cards per month (in thousand dollars)\\n8. Education: Education Level. 1: Undergrad; 2: Graduate;3: Advanced/Professional\\n9. Mortgage: Value of house mortgage if any. (in thousand dollars)\\n10. Personal_Loan: Did this customer accept the personal loan offered in the last campaign?\\n11. Securities_Account: Does the customer have securities account with the bank?\\n12. CD_Account: Does the customer have a certificate of deposit (CD) account with the bank?\\n13. Online: Do customers use internet banking facilities?\\n14. CreditCard: Does the customer use a credit card issued by any other Bank (excluding All life Bank)?\\n\"\\n## Import required libraries\\n## Define all required functions\\n## Load the dataset in dataframe from the file\\n## Understand data\\n### Check the shape of data\\n### List the columns\\n### Check the datatypes of columns\\n\"#### **Observations:**\\n\\n1. All columns are of number type (int64 / float64)\\n2. There are no columns with null value\"\\n### Check the missing data\\n\"**Observations:**\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = re.sub(r\"<.*?>\", \"\", test3)\n",
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latex math expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_all = r\"((\\$+)(?:(?!\\1)[\\s\\S])*\\1)|(\\\\begin.*?\\\\end{.*?})|(\\\\[a-zA-Z]+)\"\n",
    "regex = r\"(\\$+)(?:(?!\\1)[\\s\\S])*\\1\"\n",
    "regex1 = r\"\\\\begin.*?\\\\end{.*?}\"\n",
    "regex2 = r\"\\\\[a-zA-Z]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = '''## The number of cases in Japan at prefecture level\n",
    "## Metadata of prefectures in Japan\n",
    "\"# Grouping by growth factor<a id=\"\"2\"\"></a>\n",
    "The number of confirmed cases is increasing in many countries, but there are two of countries. In a first-type country, growth factor is larger than 1 and the number of cases is rapidly increasing. In a second-type country, growth factor is less than 1.\"\n",
    "\"## Calculate growth factor\n",
    "Where $C$ is the number of confirmed cases,  \n",
    "$$\\mathrm{Growth\\ Factor} = \\cfrac{\\Delta \\mathrm{C}_{n}}{\\Delta \\mathrm{C}_{n-1}}$$\"\n",
    "\"## Grouping countires based on growth factor\n",
    "* Outbreaking: growth factor $>$ 1 for the last 7 days\n",
    "* Stopping: growth factor $<$ 1 for the last 7 days\n",
    "* At a crossroad: the others\"\n",
    "\"## Group 1: Outbreaking, growth factor $>$ 1 for the last 7 days\"\n",
    "\"## Group 2: Stopping, growth factor $<$ 1 for the last 7 days\"\n",
    "\"## Group 3: At a crossroad, the others\"\n",
    "\"# SIR to SIR-F<a id=\"\"4\"\"></a>'''\n",
    "\n",
    "test2 = '''Model:  \n",
    "\\begin{align*}\n",
    "\\mathrm{S} \\overset{\\beta I}{\\longrightarrow} \\mathrm{I} \\overset{\\gamma}{\\longrightarrow} \\mathrm{R}  \\\\\n",
    "\\end{align*}\n",
    "\n",
    "$\\beta$: Effective contact rate [1/min]  \n",
    "$\\gamma$: Recovery(+Mortality) rate [1/min]  \n",
    "\n",
    "Ordinary Differential Equation (ODE):  \n",
    "\\begin{align*}\n",
    "& \\frac{\\mathrm{d}S}{\\mathrm{d}T}= - N^{-1}\\beta S I  \\\\\n",
    "& \\frac{\\mathrm{d}I}{\\mathrm{d}T}= N^{-1}\\beta S I - \\gamma I  \\\\\n",
    "& \\frac{\\mathrm{d}R}{\\mathrm{d}T}= \\gamma I  \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where $N=S+I+R$ is the total population, $T$ is the elapsed time from the start date.\"\n",
    "\"### Non-dimensional SIR model\n",
    "To simplify the model, we will remove the units of the variables from ODE.\n",
    "\n",
    "Set $(S, I, R) = N \\times (x, y, z)$ and $(T, \\beta, \\gamma) = (\\tau t, \\tau^{-1} \\rho, \\tau^{-1} \\sigma)$.  \n",
    "\n",
    "This results in the ODE  \n",
    "\\begin{align*}\n",
    "& \\frac{\\mathrm{d}x}{\\mathrm{d}t}= - \\rho x y  \\\\\n",
    "& \\frac{\\mathrm{d}y}{\\mathrm{d}t}= \\rho x y - \\sigma y  \\\\\n",
    "& \\frac{\\mathrm{d}z}{\\mathrm{d}t}= \\sigma y  \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where $N$ is the total population and $\\tau$ is a coefficient ([min], is an integer to simplify).  \n",
    "\n",
    "The range of variables and parameters:  \n",
    "\\begin{align*}\n",
    "& 0 \\leq (x, y, z, \\rho, \\sigma) \\leq 1  \\\\\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "& 1\\leq \\tau \\leq 1440  \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Basic reproduction number, Non-dimentional parameter, is defined as  \n",
    "\\begin{align*}\n",
    "R_0 = \\rho \\sigma^{-1} = \\beta \\gamma^{-1}\n",
    "\\end{align*}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## The number of cases in Japan at prefecture level\\n## Metadata of prefectures in Japan\\n\"# Grouping by growth factor<a id=\"\"2\"\"></a>\\nThe number of confirmed cases is increasing in many countries, but there are two of countries. In a first-type country, growth factor is larger than 1 and the number of cases is rapidly increasing. In a second-type country, growth factor is less than 1.\"\\n\"## Calculate growth factor\\nWhere  is the number of confirmed cases,  \\n\"\\n\"## Grouping countires based on growth factor\\n* Outbreaking: growth factor  1 for the last 7 days\\n* Stopping: growth factor  1 for the last 7 days\\n* At a crossroad: the others\"\\n\"## Group 1: Outbreaking, growth factor  1 for the last 7 days\"\\n\"## Group 2: Stopping, growth factor  1 for the last 7 days\"\\n\"## Group 3: At a crossroad, the others\"\\n\"# SIR to SIR-F<a id=\"\"4\"\"></a>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = re.sub(regex, \"\", test1)\n",
    "document = re.sub(regex1, \"\", document)\n",
    "document = re.sub(regex1, \"\", document)\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Model:  : Effective contact rate [1/min]  : Recovery(+Mortality) rate [1/min]  Ordinary Differential Equation (ODE):  Where  is the total population,  is the elapsed time from the start date.\"\"### Non-dimensional SIR modelTo simplify the model, we will remove the units of the variables from ODE.Set  and .  This results in the ODE  Where  is the total population and  is a coefficient ([min], is an integer to simplify).  The range of variables and parameters:  Basic reproduction number, Non-dimentional parameter, is defined as  '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = test2.replace(\"\\b\", \"\\\\b\")\n",
    "temp = temp.replace(\"\\n\", \"\")\n",
    "#document = re.sub(r\"\\\\begin.*?\\\\end{.*?}\", \"\", temp)\n",
    "#document = re.sub(r\"\\\\[a-zA-Z]+\", \"\", document)\n",
    "\n",
    "\n",
    "document = re.sub(regex, \"\", temp)\n",
    "document = re.sub(regex1, \"\", document)\n",
    "document = re.sub(regex2, \"\", document)\n",
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex1 = r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '''**Important Links -**\n",
    "\n",
    "*   Problem Statement - [Google Docs](https://drive.google.com/file/d/11BHkUYAI302GXwFaNf1B74kkmVZMM1nk/view?usp=sharing)\n",
    "\n",
    "*  Colab Notebook Link - [11263_HR_Analytics.ipynb](https://colab.research.google.com/drive/1fkRYAUnGrmo3PUGYWv7nwiHibWdR0PFi?usp=sharing)\n",
    "\n",
    "*   Datasets - [Directory Link](https://drive.google.com/drive/folders/1RyuYY_zHwdu35kxIKdmfIymzQSGIYNbw?usp=sharing)\n",
    "\n",
    "*   Libraries Pre-requisites -  [requirements.txt](https://drive.google.com/file/d/1pl9ApbHVcVneEbEZANEl8YNgo-UcUmjY/view?usp=sharing)        \n",
    "\n",
    "*   Download Pre-loaded Model -  [Pickle Link](https://drive.google.com/file/d/1hDE7SdVHxKrSKNKxGeGOb1oE8NZW8cW4/view?usp=sharing)\n",
    "\n",
    "*   Download Test Submission File -  [Submission_Test_11263.csv]()\n",
    "```\n",
    "The Below Links are not yet updated\n",
    "```'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Important Links -**\\n\\n*   Problem Statement - [Google Docs]()\\n\\n*  Colab Notebook Link - [11263_HR_Analytics.ipynb]()\\n\\n*   Datasets - [Directory Link]()\\n\\n*   Libraries Pre-requisites -  [requirements.txt](-UcUmjY/view?usp=sharing)        \\n\\n*   Download Pre-loaded Model -  [Pickle Link]()\\n\\n*   Download Test Submission File -  [Submission_Test_11263.csv]()\\n```\\nThe Below Links are not yet updated\\n```'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = re.sub(regex1, \"\", test)\n",
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'\\S*@\\S*\\s?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '''\n",
    "source\n",
    "\"#  **HR Analytics**\n",
    "\n",
    "* Author: Rahul Bordoloi \n",
    "* Email-ID : <rahul.bordoloi@highradius.com>, <mail@rahulbordoloi.me>                    \n",
    "* Emp ID: 11263                                                             \n",
    "* Date Created: 20 July, 2020      \n",
    "* Language & Version - Python 3.8.4                                      \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsource\\n\"#  **HR Analytics**\\n\\n* Author: Rahul Bordoloi \\n* Email-ID :                    \\n* Emp ID: 11263                                                             \\n* Date Created: 20 July, 2020      \\n* Language & Version - Python 3.8.4                                      \\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = re.sub(regex, \"\", test)\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
