{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI4Code: merge data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.contrib.concurrent import process_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAINS = None\n",
    "DATA_DIR = Path(\"../../data/raw\")\n",
    "if NUM_TRAINS is not None:\n",
    "    train_paths = list((DATA_DIR / \"train\").glob(\"*.json\"))[:NUM_TRAINS]\n",
    "else:\n",
    "    train_paths = list((DATA_DIR / \"train\").glob(\"*.json\"))\n",
    "\n",
    "test_paths = list((DATA_DIR / \"test\").glob(\"*.json\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_notebook(glob_path):\n",
    "    return (\n",
    "        pd.read_json(glob_path, dtype={\"cell_type\": \"category\", \"source\": \"str\"})\n",
    "        .assign(id=glob_path.stem)\n",
    "        .rename_axis(\"cell_id\")\n",
    "    )\n",
    "\n",
    "\n",
    "def merge_notebooks(notebooks_list):\n",
    "    return (\n",
    "        pd.concat(notebooks_list)\n",
    "        .set_index(\"id\", append=True)\n",
    "        .swaplevel()\n",
    "        .sort_index(level=\"id\", sort_remaining=False)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_example(id, is_train=True):\n",
    "    \"\"\"\n",
    "    Helper for loading json file of a training example\n",
    "    \"\"\"\n",
    "    filedir = \"train\" if is_train else \"test\"\n",
    "    with open(f\"../../data/raw/{filedir}/{id}.json\") as f:\n",
    "        example = json.load(f)\n",
    "    return example\n",
    "\n",
    "\n",
    "def get_example_df(example_id, train, ancestors):\n",
    "    \"\"\"\n",
    "    Creates a pandas dataframe of the json cells and correct order.\n",
    "    \"\"\"\n",
    "    cell_order = train.query(\"id == @example_id\")[\"cell_order\"].values[0]\n",
    "    example_df = pd.DataFrame(load_example(example_id))\n",
    "    example_df[\"id\"] = example_id\n",
    "    my_orders = {}\n",
    "    for idx, c in enumerate(cell_order.split(\" \")):\n",
    "        my_orders[c] = idx\n",
    "    example_df[\"order\"] = example_df.index.map(my_orders)\n",
    "    example_df.reset_index().rename(columns={\"index\": \"cell\"})\n",
    "\n",
    "    example_df[\"ancestor_id\"] = ancestors.query(\"id == @example_id\")[\n",
    "        \"ancestor_id\"\n",
    "    ].values[0]\n",
    "    example_df[\"parent_id\"] = ancestors.query(\"id == @example_id\")[\"parent_id\"].values[\n",
    "        0\n",
    "    ]\n",
    "    example_df = example_df.reset_index().rename(columns={\"index\": \"cell\"})\n",
    "    example_df = example_df.sort_values(\"order\").reset_index(drop=True)\n",
    "    example_df[\"id\"] = example_id\n",
    "    col_order = [\n",
    "        \"id\",\n",
    "        \"cell\",\n",
    "        \"cell_type\",\n",
    "        \"source\",\n",
    "        \"order\",\n",
    "        \"ancestor_id\",\n",
    "        \"parent_id\",\n",
    "    ]\n",
    "    example_df = example_df[col_order]\n",
    "    return example_df\n",
    "\n",
    "\n",
    "def combine_train():\n",
    "    train = pd.read_csv(\"../../data/raw/train_orders.csv\")\n",
    "    ancestors = pd.read_csv(\"../../data/raw/train_ancestors.csv\")\n",
    "\n",
    "    # Get the list of json files\n",
    "    train_jsons = os.listdir(\"../../data/raw/train/\")\n",
    "    print(f\"There are {len(train_jsons)} training json files\")\n",
    "\n",
    "    all_ids = train[\"id\"].unique()\n",
    "    args = ((ids, train, ancestors) for ids in all_ids[1:])\n",
    "    \n",
    "    if os.path.isfile(\"../../data/preprocessed/train_all.csv\"):\n",
    "        os.remove(\"../../data/preprocessed/train_all.csv\")\n",
    "    \n",
    "    df = get_example_df(all_ids[0], train, ancestors).reset_index(drop=True)\n",
    "    df.to_csv(\"../../data/preprocessed/train_all.csv\", index=False)\n",
    "\n",
    "    for arg in tqdm(args, desc=\"Concat dataset\", total=len(all_ids[1:])):\n",
    "        df = get_example_df(*arg).reset_index(drop=True)\n",
    "        df.to_csv(\"../../data/preprocessed/train_all.csv\", mode=\"a\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 139256 training json files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concat dataset: 100%|██████████| 139255/139255 [1:24:29<00:00, 27.47it/s]\n"
     ]
    }
   ],
   "source": [
    "combine_train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
