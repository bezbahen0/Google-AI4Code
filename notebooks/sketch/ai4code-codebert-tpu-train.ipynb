{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%cd /kaggle/working\n!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%cd /kaggle/input/googleai4codesource/Google-AI4Code/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!python -m src.clean --data /kaggle/input/googleai4codemerged/train_all.parquet --output /kaggle/working/train_cleaned.parquet --clear code","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!python -m src.featurize \\\n#            --data /kaggle/working/train_cleaned.parquet \\\n#            --output /kaggle/working/transformer_data.parquet \\\n#            --task transformer \\\n#            --features_out_path /kaggle/working/transformer_features.json \\\n#            --num_selected_code_cells 20 \\\n#            --mode train ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!apt-get install -y libomp5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nfrom transformers import AutoModel, AutoTokenizer\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nclass TransformersModel(nn.Module):\n    def __init__(self, model_path):\n        super(TransformersModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_path)\n        self.top = nn.Linear(769, 1)\n        \n    def forward(self, ids, mask, fts):\n        x = self.model(ids, mask)[0]\n        x = torch.cat((x[:, 0, :], fts),1)\n        x = self.top(x)\n        return x\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch_xla\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.utils.utils as xu\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.test.test_utils as test_utils\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport gc\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport torch\nimport pickle\nimport numpy as np\n\nfrom transformers import AutoModel, AutoTokenizer\nfrom torch.utils.data import Dataset\n\n\nclass XGBrankerDataSet:\n    def __init__(self, data_path):\n        self.data_path = data_path\n\n    def load_data(self):\n        with open(self.data_path, \"rb\") as input_file:\n            X_train, y_train, groups = pickle.load(input_file)\n        return X_train, y_train, groups\n\n\nclass TransformersDataset(Dataset):\n    def __init__(self, data_path):\n        super().__init__()\n        self.data_path = data_path\n\n    def load_data(self):\n        with open(self.data_path, \"rb\") as f:\n            self.ids = np.load(f)\n            self.masks = np.load(f)\n            self.fts = np.load(f)\n            self.ranks = np.load(f)\n\n    def __getitem__(self, index):\n        return (\n            torch.from_numpy(self.ids[index]),\n            torch.from_numpy(self.masks[index]),\n            torch.FloatTensor([self.fts[index]]),\n            torch.FloatTensor([self.ranks[index]]),\n        )\n\n    def __len__(self):\n        return self.ids.shape[0]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_fn(vals):\n    # take average\n    return sum(vals) / len(vals)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport logging\nimport argparse\nimport torch\nimport numpy as np\n\nfrom torch.utils.data import DataLoader\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\nfrom tqdm import tqdm\n\n\ndef to_device(data):\n    return [d.to(device, dtype=torch.long) for d in data[0]], data[1].to(device, dtype=torch.float)\n\n\ndef train_transformer(\n    data_path,\n    output_model_path,\n    model_name_or_path,\n    accumulation_steps,\n    batch_size,\n    epochs,\n    n_workers,\n):\n    np.random.seed(0)\n    \n    train_data = TransformersDataset(\n        data_path,\n    )\n\n    train_data.load_data()\n    \n    #TPU\n    # defining data samplers and loaders \n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n          train_data,\n          num_replicas=xm.xrt_world_size(), # tell PyTorch how many devices (TPU cores) we are using for training\n          rank=xm.get_ordinal(), # tell PyTorch which device (core) we are on currently\n          shuffle=True)\n\n\n    train_loader = DataLoader(\n        train_data,\n        batch_size=batch_size,\n        sampler=train_sampler,\n        num_workers=n_workers,\n    )\n\n    model = TransformersModel(model_name_or_path)\n    model = model.to(device)\n    xm.master_print('done loading model')\n\n    # Creating optimizer and lr schedulers\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [\n                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.01,\n        },\n        {\n            \"params\": [\n                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    \n    lr = 3e-5 * xm.xrt_world_size() # scale the learning rate\n   \n    num_train_optimization_steps =  int(len(train_loader) / batch_size / xm.xrt_world_size() * epochs) \n    #num_train_optimization_steps = int(epochs * len(train_loader) / accumulation_steps)\n    optimizer = AdamW(\n        optimizer_grouped_parameters, lr=lr, correct_bias=False\n    )  # To reproduce BertAdam specific behavior set correct_bias=False\n\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_train_optimization_steps,\n    )  # PyTorch scheduler\n\n    criterion = torch.nn.L1Loss()\n\n    for e in range(epochs):\n        gc.collect()\n        model.train()\n        para_loader = pl.ParallelLoader(train_loader, [device]) \n        tbar = tqdm(para_loader.per_device_loader(device))\n        loss_list = []\n        preds = []\n        labels = []\n\n        for idx, data in enumerate(tbar):\n            ids, mask, fts, target = data\n            \n            optimizer.zero_grad()\n            ids = ids.to(device)\n            mask = mask.to(device)\n            fts = fts.to(device)\n            target = target.to(device)\n            \n            pred = model(ids, mask, fts)\n            loss = criterion(pred, target)\n            \n            loss.backward()\n            if idx % accumulation_steps == 0 or idx == len(tbar) - 1:\n                xm.optimizer_step(optimizer, barrier=True)\n                scheduler.step()\n                \n            loss_reduced = xm.mesh_reduce('loss_reduce',loss,reduce_fn) \n         \n            tbar.set_description(\n                f\"Epoch {e + 1} Loss: {loss_reduced} lr: {scheduler.get_last_lr()}\"\n            )\n\n        torch.save(model.state_dict(), output_model_path)    \n\ntrain_transformer(\n    '/kaggle/input/googleai4codemerged/transformer_data.npy',\n    '/kaggle/working/trained_mode.bin',\n    'microsoft/codebert-base',\n    4,\n    10,\n    5,\n    8,\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}